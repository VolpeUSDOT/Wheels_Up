---
title: "BTS Flight Time Regression Results"
output: 
  html_document:
    self_contained: true
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
---

<!-- saved from url=(0014)about:internet -->
<!-- This needs to be manually added to the second line of the html -->

# Overview 

Summary of results from models of all reportable flights from 2015 - 2018. 

Two modeling approaches were taken. First, individually modeling each flight segment (origin-destination, O-D) by each carrier (C):

$$
AIRTIME_{C_{O-D}} = \beta1_{C_{O-D}} Year_{C_{O-D}} + \beta2_{C_{O-D}} MONTH_{C_{O-D}} + \beta3_{C_{O-D}}DAY_{C_{O-D}} + \beta4_{C_{O-D}}DEP_{C_{O-D}}
$$

For each carrier, each origin-destination flight segment was modeled as a linear combination of month of year, day of week, and departure time block (hour of day, with midnight to 5am as one time block). These are the variables that an air carrier would know when scheduling a flight, and thus what would be available to provide a prediction of estimated flight time for a consumer at the time of booking. Two-way interactions were included. 

Second, individually modeling each flight segment (origin-destination, O-D) across carrier:

$$
AIRTIME_{O-D} = \beta1_{O-D} CARRIER_{O-D} + \beta2_{O-D} YEAR_{O-D} + \beta3_{O-D} MONTH_{O-D} + \beta4_{O-D}DAY_{O-D} + \beta5_{O-D}DEP_{O-D}
$$

This approach differs from the first by including all flights on a particular flight segment regardless of carrier. The carrier-specific effects are then an offset from the overall estimate, with two--way interactions again included. 

Additionally, we tested how adding congestion at the airport and hour level improve model performance. For this, we calculated the count of flights departing or arriving at each of the 320 airports included in these analyses, for each year, month, day of week, and time block (hour of day, with midnight to 5am as one time block). Within each airport and year, we then calculated a relative congestion, as the specific number of flights within a time block divided by the maximum number of flights for that airport and year. This provide a measure of proportionally how busy the origin and destination airports were, relative to their maximum capacity at any point in the year. 

Finally, we also tested a machine learning approach to flight time prediction. Similar to a linear regression approach, a matrix of predictors is fit to an target variable (flight time). The particular approach selected, extreme gradient boosting (XGBoost) is a decision-tree approach which has been highly effective at solving a number of data science problems, and used in a number of the studies reviewed in our Annotated Bibliography.

## Validation

In order to validate a predictive model, a model has to be fit using one portion of the data, and then applied to a new set of data which was not used in the fitting process. For this project, we are focused on assessing the feasibility for a carrier to report air time for a flight segment at the time of booking. Therefore, we want to know how good the models we fit are at generating air time estimates. We can validate first internally, using a portion of the data from 2015-2018 which was not used in the fitting of the model, as well as externally, applying the models fitted with complete 2015-2018 data to flights in 2019.

- Internal validation: Set aside a stratified random sample of flights within each O-D pair flown by each carrier. Use 80% of the flight data to train the model, and the remaining 20% is set aside to test the model.
- External validation: Use the complete 2015-2018 data to fit the model. Use available flights from 2019 to test the model. 

When preparing these models, only January and February flight data were available for 2019 ([Transtats](https://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline On-Time Performance Data&DB_Short_Name=On-Time)).  

The most important metric of model goodness-of-fit for this project is how close the model predictions are to the observed air time. We use mean absolute error (MAE) to show how close model predictions are in minutes to observed air times for a given O-D pair. MAE is calculated as follows:

$$
MAE = \frac{ \sum^{n}_{i=1}|\hat{y}_i - y_i | }{n}
$$
For $n$ observations of an O-D pair $i$, where $y_i$ is the observed air time and $\hat{y}_i$ is the predicted flight time. In addition, it is useful to report the coefficient of determination for a model, or $r^2$. Unlike MAE, which is reported in the original unit of minutes of air time, $r^2$ ranges from 0-1 and reflects the proportion of the variation in air time which is explained by the model. For this project, $r^2$ is useful for comparing across models, but MAE is most relevant for the determination of feasibility of flight time reporting. 

## Model summary

Combining the two modeling approaches (O-D within carrier and O-D across carriers) and two validation approaches (internal and external) results in the following combinations of models.

| Model Approach     | Validation Approach | Training Data Set                        | Test Data Set                          | Total Number of Models |
|--------------------|---------------------|------------------------------------------|----------------------------------------|------------------------|
| O-D Within Carrier | Internal            | 2015-2018, 80% of data<br>17.5 M flights | 2015-2018, 20% of data<br>4.4 M flights|     10,372             |
| O-D Within Carrier | 2019                | 2015-2018, 100% of data<br>21.9 M flights| 2019 available data<br>861,623 flights |     10,372             |
| O-D Across Carrier | Internal            | 2015-2018, 80% of data<br>17.5 M flights | 2015-2018, 20% of data<br>4.4 M flights|      5,101             |
| O-D Across Carrier | 2019                | 2015-2018, 100% of data<br>21.9 M flights| 2019 available data<br>861,623 flights |      5,101             |

The O-D within carrier approach yields more total models because multiple carriers fly the same O-D flight segment, for 2,779 of the 5,101 segments. Note that only flight segments which appeared in the 2018 data were included; thus, flight segments which were only flown in 2015 but not in 2018 for example would have been excluded.

<!-- 
summary(table(d_within_Internal$O_D) == 1) # to see how many flight segments are flown by just a single carrier
load(file.path(sharedloc, 'ASQP_2019_validate.RData')); format(nrow(d_19), big.mark = ',') # to look at 2019 data used to date
load(file.path(sharedloc, 'ASQP_2015-2018_train.RData')); 
format(nrow(d_crossyear_train), big.mark = ',')
# "17,532,876"
format(nrow(d_crossyear_validate), big.mark = ',')
# "4,378,035"
format(sum(nrow(d_crossyear_validate), nrow(d_crossyear_train)), big.mark = ',')
# "21,910,911"
-->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning=F, message=F)

library(tidyverse)
library(knitr)
library(DT)
library(kableExtra)
library(plotly) # note: masks filter() from dplyr

codeloc = ifelse(grepl('Flynn', normalizePath('~/')),
                   "~/git/Wheels_Up",
                   "~/GitHub/Wheels_Up")
sharedloc = "//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/BTS_Flight_performance/Data"
resultsloc = "//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/BTS_Flight_performance/Results"

load(file.path(resultsloc, 'All_Results.RData'))

```


<!--
TODO:
- Report percent error, e.g. d_ensemble_2019 %>% mutate(pct_err = 100* (MAE/Obs_Mean)) %>% summarize(mean(pct_err, na.rm=T)) # about 5.4% error
- R2 by N, R2 by mean flight time, and r2 by sd flight time plots
- Table of most and least predictable flights
-->

## Conclusions

- The relatively simple models here demonstrate that carriers can be reasonably expected to predict air times within a 10 minute range for the majority of flights. The median error across all flights predicted in 2019 was `r round(median(d_ensemble_2019$MAE, na.rm=T), 2)` minutes (O-D across carriers, validation on 2019 data). `r round(d_ensemble_2019 %>% filter(!is.na(MAE)) %>% summarize(pct_within_10 = 100*sum(MAE <= 10)/n()), 1)`% of flights were predicted within a 10 minute margin of error. 

- Expressed as a percent of the actual flight time, across all flights in 2019, the models had a `r round(d_ensemble_2019 %>% mutate(pct_err = 100* (MAE/Obs_Mean)) %>% summarize(mean(pct_err, na.rm=T)), 2)`% error in flight time.

- Validation on 2019 data shows higher MAE compared to internal validation, which is expected. For example, the median MAE for American Airlines flights when validated internally to the 2015-2018 data was 6.21 minutes, for 842 O-D pairs. The median MAE for American Airlines flights when validated on the 2019 data was 7.39 minutes, for 662 O-D pairs. The further into the future the flight time is predicted, the greater the error in the prediction. 

- Median $r^2$ was around 0.3. Thus, while the accuracy of the flight times on average for an O-D pair is fairly high, a substantial degree of variation in flight times is not captured in these models, which is again to be expected. Incorporating weather variables could substantially increase the $r^2$ for historical flight time models, but would not be useful for predicting flight time at the time of booking (except for consumers who book within a few days of departure).

- Modeling flight time across carriers rather than within each carrier yielded better performance (median MAE across = `r round(median(d_ensemble_2019$MAE, na.rm=T), 2)` minutes; median MAE within = `r round(median(d_within_2019$MAE, na.rm=T), 2)` minutes). This indicates that there is useful information from other carriers flying the same flight segment when predicting flight time for a specific flight.

- Adding congestion variables improved the predictive ability of the models substantially. Across all 5,101 O-D pairs modeled, congestion variables were included in approximately half of the final models.

- The machine learning algorithm XGBoost provided the best estimates of flight time for a minority of O-D pairs, 794 using the 2019 flight data validation set. Within the XGBoost models, the most important variables were typically the relative congestion at the destination airport (297 O-D pairs) and relative congestion at the origin airport (242 O-D pairs).

- Of all 5,101 O-D pairs, there was insufficient data on 101 flight segments when validating with across all available data (2015-2018). Using just the 2019 flight data available to date, 1,179 O-D pairs had insufficient data. 

# Summary of best model ensemble 


The following figures summarize the modeled air time error in two ways. First, the MAE (in minutes) is plotted against the observed mean air time for each individual O-D pair in the validation set. Second, the percent error in air time is plotted, i.e., MAE/Observed mean air time. These figures use models trained on 80% of the 2015-2018 data, and validated on the remaining 20%; the figures for the models validated on 2019 data are similar.

These figures illustrate that the feasibility of predicting air time is high if a 10 minute or 10% error is acceptable. At more strict thresholds, fewer flight segments could be reliably predicted.

```{r mae_fig1}

ggplot(d_ensemble_Internal) +
  geom_point(aes(x = Obs_Mean, y = MAE, size = N), color = 'midnightblue', alpha = 0.1) +
  theme_bw() +
  xlab('Mean Air Time (minutes)') + ylab('Model error in air time (MAE, in minutes)') +
  ggtitle('Model error for each O-D pair. \n Size of points represents number of flights.')

ggplot(d_ensemble_Internal %>% mutate(pct_Err = 100*(MAE / Obs_Mean))) +
  geom_point(aes(x = Obs_Mean, y = pct_Err, size = N), color = 'firebrick4', alpha = 0.1) +
  theme_bw() +
  xlab('Mean Air Time (minutes)') + ylab('Error in air time (MAE/Observed Mean, %)') +
  ggtitle('Percent model error for each O-D pair. \n Size of points represents number of flights.')


```


```{r predictibility_plot}
d_ensemble_2019 = d_ensemble_2019 %>% mutate(pct_Err = MAE/Obs_Mean)
d_ensemble_Internal = d_ensemble_Internal %>% mutate(pct_Err = MAE/Obs_Mean)

min_thresholds = seq(1:20)
min_pred = vector()
for(i in min_thresholds){
  min_pred = c(min_pred, nrow(d_ensemble_Internal %>% filter(MAE <= i)))
}

min_thresholds = data.frame(min_thresholds, min_pred)
min_thresholds = min_thresholds %>% mutate(pct_pred = 100*min_pred/nrow(d_ensemble_2019))

ggplot(min_thresholds) +
  theme_bw() +
  geom_line(aes(x = min_thresholds, y = pct_pred), col = 'midnightblue', lwd = 2) +
  ylab('Percent of all flight segments which can be reliably predicted') +
  xlab('Threshold of MAE cutoff (minutes)') + 
  ggtitle('Predictability of flight segments across various MAE thresholds')


pct_thresholds = seq(from = 0.01, to = 0.2, by = 0.01)
pct_pred = vector()
for(i in pct_thresholds){
  pct_pred = c(pct_pred, nrow(d_ensemble_Internal %>% filter(pct_Err <= i)))
}

pct_thresholds = data.frame(pct_thresholds = 100*pct_thresholds, pct_pred)
pct_thresholds = pct_thresholds %>% mutate(pct_fl_pred = 100*(pct_pred / nrow(d_ensemble_2019)))

ggplot(pct_thresholds) +
  theme_bw() +
  geom_line(aes(x = pct_thresholds, y = pct_fl_pred), col = 'firebrick4', lwd = 2) +
  ylab('Percent of all flight segments which can be reliably predicted') +
  xlab('Threshold of percent error cutoff (%)') + 
  ggtitle('Predictability of flight segments across various percent error thresholds')

```

The following table summarizes the predictability of flights for the ensemble of the best fitting model for each O-D pair. 

## Internal Validation (2015-2018) summarized by airport

```{r airport_summary_ensemble1}
d_airport <- d_ensemble_Internal %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for best model by each O-D pair, internal validation 2015-2018. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```



## External Validation (2019) summarized by airport

```{r airport_summary_ensemble2}
d_airport <- d_ensemble_2019 %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for best model by each O-D pair, external validation on 2019 data. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```



# Summary of all models

The following tables summarizes the predictability of flights, for each individual set of models: base regression models, regression models with congestion added, and XGBoost machine learning models.

For the O-D within carrier approach, we report the total number of unique flight segments (origin-destination pairs) modeled, as well as the MAE and $r^2$ for across all flights for that carrier. Not all flight segments could be reliably modeled; for instance, American flew only a single flight between DCA and Atlanta in 2018 (on 2018-12-23, at 9pm). With only a single value for this combination of carrier and origin-destination pair, the current modeling framework does not produce an estimate for this flight segment.

For both approaches (O-D within carrier and O-D across carrier), we summarize the model performance by MAE and $r^2$ for each airport. A Tableau dashboard will provide more complete exploration of these results.


## O-D Within Carrier, base regression

### Internal Validation (2015-2018) summarized by carrier

```{r carr_summary1}
d_carrier <- d_within_Internal %>%
  group_by(carrier) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm = T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3),
            #R2_max = round(max(r2[r2 != 1], na.rm = T), 3),
            #Pct_High_R2 = round(100*sum(!is.na(r2) & r2 > 0.5) / length(r2), 2)
            )

kable(d_carrier,
      format.args = list(big.mark = ','),
      caption = "Summary of flight predictability by carrier, 2015-2018 with internal validation. MAE: Mean absolute error in minutes for each flight segment." ) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

```

### Internal Validation (2015-2018) summarized by airport

```{r airport_summary1}
d_airport <- d_within_Internal %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '200 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for internal validation 2015-2018. Airports with at least 200 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```

### External Validation (2019) summarized by carrier

**Note** Virgin Atlantic (VX) merged with Alaskan Airlines in 2018, and thus had no reportable flights in 2019. 

```{r carr_summary2}
d_carrier <- d_within_2019 %>%
  group_by(carrier) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm = T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3),
            #R2_max = round(max(r2[r2 != 1], na.rm = T), 3),
            #Pct_High_R2 = round(100*sum(!is.na(r2) & r2 > 0.5) / length(r2), 2)
            )

kable(d_carrier,
      format.args = list(big.mark = ','),
      caption = "Summary of flight predictability by carrier, using external validation on flights in 2019. MAE: Mean absolute error in minutes for each flight segment." ) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

```


### External Validation (2019) summarized by airport

```{r airport_summary2}
d_airport <- d_within_2019 %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for external validation on 2019 data. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```


## O-D Across Carrier, base regression models

### Internal Validation (2015-2018) summarized by airport

```{r airport_summary3}
d_airport <- d_across_Internal %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for models across carriers, internal validation 2015-2018. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```

### External Validation (2019) summarized by airport

```{r airport_summary4}
d_airport <- d_across_2019 %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for models across carriers, external validation on 2019 data. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```


# Additional analyses

Two additional analyses are described here. First, relative congestion at each airport as an origin and destination was calculated by year, month, day of week, and time block. For 2019 data validation, the 2018 relative congestion values were used, since a complete year of data is needed to calculate relative congestion. Second, results from the machine learning approach, XGBoost, are presented and compared with the base regression approach.  

## O-D Across Carrier, regression with congestion variables

Compare to models without these two variables added, models with these variables were substantially better for many flights. For flight segments which are flown less frequently, adding these variables induced model overfitting in some cases, reducing the predictive power of the models. For points to the left of the dashed 1:1 line, MAE was higher (worse) for models with congestion variables added. For points to the right of the dashed 1:1 line, MAE was lower (better) for models with congestion variables added. 

```{r cong_in_plot}

d_cong <- d_across_Internal
d_cong <- left_join(d_cong,
                    d_across_cong_Internal %>% rename(MAE_cong = MAE) %>% select(O_D, MAE_cong)) %>%
  mutate(high_flights = factor(N > 1000, labels = c('< 1,000', '>= 1,000')))


ggplot(d_cong, aes(x = MAE, y = MAE_cong)) + 
  geom_point(alpha = 1/10, col = 'midnightblue') +
   ylim(0, 100) + xlim(0, 100) +
  facet_wrap(~high_flights)+#, scales = 'free_y') +
  xlab("MAE base regression models") + ylab("MAE regression + congestion variables") +
  geom_abline(slope = 1, intercept = 0, lty = 3) +
  theme_bw() +
  ggtitle("MAE with congestion variables added, validation internally to 2015-2018 data. \n Separating O-D pairs with fewer or greater than 1,000 flight segments.")

```

### Congestion: Internal Validation by airport

```{r airport_summary_in_cong}
d_airport <- d_across_cong_Internal %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for models across carriers including congestion at origin and destination, internal validation 2015-2018. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```

### Congestion: External Validation by airport

```{r airport_summary_ext_cong}
d_airport <- d_across_cong_2019 %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '50 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for models across carriers including congestion at origin and destination, external validation on 2019 data. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```

Similarly to the internal validation, the use of the congestion variables improved the predictive ability of the models when validated on 2019 data in many cases. 

```{r cong_ext_plot}

d_cong <- d_across_2019
d_cong <- left_join(d_cong,
                    d_across_cong_2019 %>% rename(MAE_cong = MAE) %>% select(O_D, MAE_cong)) %>%
  mutate(high_flights = factor(N > 1000, labels = c('< 1,000', '>= 1,000')))


ggplot(d_cong, aes(x = MAE, y = MAE_cong)) + 
  geom_point(alpha = 1/10, col = 'midnightblue') +
  ylim(0, 100) + xlim(0, 100) +
  facet_wrap(~high_flights)+#, scales = 'free_y') +
  geom_abline(slope = 1, intercept = 0, lty = 3) +
  theme_bw() +
  xlab("MAE base regression models") + ylab("MAE regression + congestion variables") +
  ggtitle("MAE with congestion variables added, validation on 2019 data. \n Separating O-D pairs with fewer or greater than 1,000 flights")

```

## O-D Across Carrier, XGBoost machine learning models

### Internal Validation by airport

```{r airport_summary_in_xgb}
d_airport <- d_across_xgb_Internal %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for XGBoost models across carriers, internal validation 2015-2018. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```

The XGBoost models improved model performance for many flight segments with fewer than 1,000 flights flown. Namely, MAE of XGBoost models was lower than that of linear regression models, i.e. to the right of the 1:1 line in the plot below.  However, for O-D pairs of 1,000 or more flights in the training data, XGBoost models typically did not improve model performance.  

```{r xgb_in_plot}
d_c <- left_join(d_across_Internal,
                 d_across_xgb_Internal %>% rename(MAE_xgb = MAE) %>% select(O_D, MAE_xgb)) %>%
  mutate(high_flights = factor(N > 1000, labels = c('< 1,000', '>= 1,000')))

ggplot(d_c, aes(x = MAE, y = MAE_xgb)) + 
  geom_point(alpha = 1/10, col = 'midnightblue') +
  ylim(0, 40) + xlim(0, 40) +
  facet_wrap(~high_flights)+#, scales = 'free_y') +
  geom_abline(slope = 1, intercept = 0, lty = 3) +
  theme_bw() +
  xlab("MAE base regression models") + ylab("MAE XGBoost") +
  ggtitle("MAE with XGBoost, validation internally to 2015-2018 data. \n Separating O-D pairs with fewer or greater than 1,000 flights.")

```


### External Validation by airport

```{r airport_summary_ext_xgb}
d_airport <- d_across_xgb_2019 %>%
  group_by(Origin) %>%
  filter(!is.na(MAE)) %>%
  summarize(N_flight_segments = n(),
            N_estimated = sum(!is.na(r2)),
            MAE_median = round(median(MAE, na.rm=T), 2),
            MAE_min = round(min(MAE, na.rm = T), 2),
            MAE_max = round(max(MAE, na.rm = T), 2),
            R2_median = round(median(r2, na.rm=T), 3)
            )

datatable(d_airport,
          rownames = F,
          filter = 'top',
          options = list(order = list(list(3, 'asc')),
                        searchCols = list(NULL, list(search = '100 ... 1000'), NULL, NULL, NULL, NULL, NULL),
                         pageLength = 10),
      caption = "Summary of flight predictability by origin airport, for XGBoost models across carriers, external validation on 2019 data. Airports with at least 100 departing flight segments shown by default. Sorted by median MAE, small to large." ) 

```


When validating on the 2019 data, the XGBoost models did substantially improve model fit for many flight segments, including both those flown less frequently and those flown more frequently (point to the right of the 1:1 line). The ensemble presented in the top section refers to the best model for each O-D pair across all of these model approaches.



```{r xgb_ext_plot}
d_c <- left_join(d_across_2019,
                 d_across_xgb_2019 %>% rename(MAE_xgb = MAE) %>% select(O_D, MAE_xgb)) %>%
  mutate(high_flights = factor(N > 1000, labels = c('< 1,000', '>= 1,000')))

ggplot(d_c, aes(x = MAE, y = MAE_xgb)) + 
  geom_point(alpha = 1/10, col = 'midnightblue') +
  ylim(0, 40) + xlim(0, 40) +
  facet_wrap(~high_flights)+#, scales = 'free_y') +
  geom_abline(slope = 1, intercept = 0, lty = 3) +
  theme_bw() +
  xlab("MAE base regression models") + ylab("MAE XGBoost") +
  ggtitle("MAE with XGBoost, validation to 2019 data. \n Separating O-D pairs with fewer or greater than 1,000 flights.")

```
